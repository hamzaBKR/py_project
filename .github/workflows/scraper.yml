name: Web Scraper CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 9 AM UTC
    - cron: '0 9 * * *'
  workflow_dispatch:  # Allow manual triggers

jobs:
  # Job 1: Build and test with container
  scrape-with-container:
    name: Scrape Data (Container)
    runs-on: ubuntu-latest
    
    # This is KEY: we run inside our custom container
    # This ensures exact Chrome/ChromeDriver versions every time
    container:
      image: ghcr.io/${{ github.repository_owner }}/scraper:latest
      # If image doesn't exist yet, build it first (see build job)
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Run scraper
        run: python scraper.py
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results
          path: results.json
          retention-days: 30
      
      - name: Display results
        run: |
          echo "=== Scraping Results ==="
          cat results.json | python -m json.tool

  # Job 2: Build and push container image
  build-container:
    name: Build Docker Image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository_owner }}/scraper
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # Job 3: Test WITHOUT container (to show the difference)
  scrape-without-container:
    name: Scrape Data (No Container - May Fail)
    runs-on: ubuntu-latest
    continue-on-error: true  # Don't fail the workflow if this fails
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Run scraper
        run: python scraper.py
        # This might fail due to ChromeDriver version mismatches!
      
      - name: Upload results
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results-no-container
          path: results.json

  # Job 4: Compare results and generate report
  compare-results:
    name: Compare Approaches
    runs-on: ubuntu-latest
    needs: [scrape-with-container, scrape-without-container]
    if: always()
    
    steps:
      - name: Download container results
        uses: actions/download-artifact@v4
        with:
          name: scraping-results
          path: ./container-results
        continue-on-error: true
      
      - name: Download no-container results
        uses: actions/download-artifact@v4
        with:
          name: scraping-results-no-container
          path: ./no-container-results
        continue-on-error: true
      
      - name: Generate comparison report
        run: |
          echo "# Scraping Results Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "./container-results/results.json" ]; then
            echo "✅ **Container approach**: SUCCESS" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat ./container-results/results.json >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Container approach**: FAILED" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "./no-container-results/results.json" ]; then
            echo "✅ **No-container approach**: SUCCESS" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **No-container approach**: FAILED (likely ChromeDriver mismatch)" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Why Use Containers?" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Guaranteed Chrome/ChromeDriver compatibility" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Reproducible environment across all runs" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Same image used locally and in CI" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Faster execution (dependencies pre-installed)" >> $GITHUB_STEP_SUMMARY
